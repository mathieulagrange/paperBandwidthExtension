% Template for ICASSP-2020 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx,xcolor,hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    bookmarks=true,
    citecolor=magenta,
}
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

\newcommand{\ml}[1]{\textcolor{blue}{ML : #1}}
\newcommand{\fg}[1]{\textcolor{red}{FG : #1}}

% Title.
% ------
\title{Bandwidth extension of musical audio signals with no side information using dilated convolutional neural networks}
%
% Single address.
% ---------------
\name{Mathieu Lagrange, F\'elix Gontier \thanks{Work partially funded by ANR CENSE}}
\address{LS2N, CNRS, Centrale Nantes}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}

Bandwidth extension has a long history in audio processing. While speech processing tools do not rely on side information, production ready bandwidth extension tools of general audio signal rely on side information that has to be transmitted alongside the bitstream of the low frequency part, mostly because polyphonic music has a more complex and less predictable spectral structure than speech.

This paper study the benefit of considering dilated full convolutional neural network to perform the bandwidth extension of musical audio signals with no side information in the spectral domain. Experimental comparison with autoencoders, demonstrate that the proposed architecture achieves state of the art performance on two public datasets, respectively of monophonic and polyphonic music.


\end{abstract}
%
\begin{keywords}
Artificial audio bandwidth extension, deep neural network, musical audio processing
\end{keywords}
%
\section{Introduction}
\label{sec:intro}

Bandwidth extension has a long standing history in telecommunication where the bitrate allowed by the given application may be reduced \cite{larsen2005audio}. In this case, it is often beneficial to preserve a good perceptual quality in the lower frequencies, for example to preserve intelligibility for speech applications. In that case, it may interesting to have a processing unit on the receiver that is able to produce a wide-band signal in order to improve perceived quality given the narrow band signal considering the lower frequencies typically up to 4 kHz for speech,.

Many techniques have been introduced, and most of them operates in the spectral domain, where the spectral envelope of the narrow band signal is used to predict the spectral envelope of the higher frequencies. Recent approaches considers deep neural network to do so \cite{abel2017artificial}. This approach assumes a source-filter model for speech production and requires some integration of the two processing units, the one responsible for the narrow band signal decoding and the one responsible for the bandwidth extension. Considering the mapping in the spectral domain using deep neural architecture have been proposed in \cite{li2015deep}. Recurrent neural networks \cite{ling2018waveform} or Wavenet architectures can also be considered \cite{gupta2019speech}. In this case, the network directly predict the wide band speech signal, given some information provided by the conditioning stack that processes the narrow band signal. This approach is very flexible, but computationally demanding.

Due to interoperability requirements in telephony, the bandwidth extension process is done without any side information. That is, the processing unit on the decoder side has to predict the higher frequency signal given some static knowledge and the lower frequency signal only.

In general audio coding, bandwidth extension has been introduced in the beginning of the millennium \cite{dietz2002spectral}. General audio coding is more complex than speech coding due to the variety of physical sources that may produce the signal that has to be encoded. Due to this, and the ability to control the whole transmission stack for most use cases, some side information is considered to perform the bandwidth extension process. This side information is computed using the wide band signal at the encoder side and transmitted within the bitstream. In \cite{dietz2002spectral}, the main concept introduced is called spectral band replication (sbr) where the lower frequencies of the magnitude spectra are duplicated and transposed. Due to the typical exponential decay of magnitude with respect to the frequency, the overall magnitude of the transposed spectra has to be adjusted.

Some post processing steps can be undertaken to further improve the perceptual quality. As the encoder has access to the sbr prediction and the reference high frequency spectra, it is able to adapt to some special cases where considering the high frequency spectrum as the low frequency one will fail. For example,  some high frequency tones that are perceptively salient may not be be recreated using the replication process. In this case, an additional processing unit can be considered to produce salient sinusoidal components \cite{ekstrand2002bandwidth}. The lower frequencies may have strong harmonics and the higher ones only noise like components. In this case, an inverse filtering is applied \cite{ehret2004audio}.

Extension for low delay applications have been proposed \cite{friedrich2007spectral}, as well as the application of the phase vocoder \cite{flanagan1966phase} to reduce unpleasant roughness typically introduced when considering sbr tools \cite{nagel2009harmonic}.

Compared to algorithmic approaches discussed above, considering learning approaches have several benefits. First, if the capacity of the model is sufficient to encode the many relationships between the lower and the higher part of the spectrum and if those encoded relationships are generic enough to produce satisfying results for real use case scenarios, there is no need for side information. Secondly, the relationships encoded by the model being non explicit, there is less chance of reaching a "glass-ceiling" in terms of perceptual quality. Non-negative matrix factorization approaches \cite{sun2013non},  and deep learning approaches have recently been considered \cite{miron2018high}.

To investigate further in this direction, we consider in this paper to consider a deep convolutional network approach that operates in the magnitude spectrum. The model is described in Section \ref{sec:model}. Its performance is evaluated using an experimental protocol described in Section \ref{sec:protocol} on two public datasets: the \textit{medley-solos-db} and the \textit{gtzan} datasets. Outcomes of the performance analysis are described in Section \ref{sec:experiments} and discussed in Section \ref{sec:discussion}. Our main findings are that: 1) dilation allows us to regularize the performance of the predictor and reduce the complexity of the model while preserving a receptive field adapted to the task at hand, 2) compressive architectures like autoencoders do not perform favorably compared to a fully convolutional neural network. \footnote{Reproducible research statement: the experiments described in this paper rely on public data and the code used to produce the results will be made available upon publication on the companion website where some audio examples can be listened to: \url{https://mathieulagrange.github.io/paperBandwidthExtensionCnn/demo}.}

\section{Model}
\label{sec:model}

The aim of a bandwidth extension system is to predict the high frequency part. In this paper, we consider audio data represented in the spectral domain. The input and the output of the model consists in 128x10 (frequency x time) matrices that respectively represents the low frequency and high frequency parts of the audio for an approximate duration of $160$ms (more details are provided in Section \ref{sec:protocol}).

The architecture is a fully convolutional neural network \cite{long2015fully} with $L$ layers followed by rectified linear units (ReLU) activations, as described in Figure~\ref{fig:mdl}. The number of output convolution channels $C$ is the same for all hidden layers. Convolution kernels also share the same size $(K_t, K_f)$ in the time and frequency dimensions respectively. To keep the shape of magnitude spectra constant throughout the network, representations at the input of each layer are padded by replicating their boundary values depending on the kernel size. Dilated convolutions \cite{yu2016multi, oord2016wavenet} are considered to artificially increase the receptive field of the model, allowing it to capture patterns on larger scales without added parameters. A fixed dilation ratio $D$ is used in the frequency dimension for hidden layers, and no dilation is used in the input and output layers of the network. As a result each hidden layer increases the receptive field by $D(K-1)$ frequency bins compared to $K-1$ without dilation.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/mdl.pdf}
    \caption{Proposed deep convolutional neural network architecture for bandwidth extension.}\label{fig:mdl}
\end{figure}

The model is trained using the mean squared error (MSE) loss function of expression:
\begin{equation}
L(y, \hat y) = \frac{1}{N}\sum\limits_{n=1}^{N_b}\sum\limits_{t=1, f=1}^{T, F}(\hat y - y)^2
\end{equation}
where $y$ is the ground truth high frequency part of the spectrum, $\hat y$ is the output of the model, $N_b$ is the batch size, ant $T$ and $F$ are the time and frequency dimensions of the output respectively. Optimization is performed using the Adam \cite{kingma2014adam} algorithm with minibatches of $64$ examples and a learning rate of $0.001$.


\section{Experimental protocol}
\label{sec:protocol}

Two datasets are considered in this study, the \textit{gtzan} dataset \cite{tzanetakis2002musical}, and the \textit{medley-solos-DB} dataset \cite{lostanlen2016deep}. The first comprises 100 polyphonic pop songs of 30 seconds for each of the 10 musical genres represented for a total duration of about 8 hours. The second has 21572 audio clips of about 3 seconds, for a total duration of about 18 hours.

The \textit{gtzan} dataset is split in a training set and testing set using the following procedure. For each genre, the 70 first songs are put in the train set and the 30 remaining in the test set. The \textit{medley-solos-DB} dataset is already split into "training", and "test" datasets, they are considered as is in this study.

For both datasets, the audio data is resampled to $8$kHz and converted to spectral data using a short-term Fourier transform with frame size of $256$ samples, hop size of $128$ samples and a Hann window and split into texture windows of $10$ frames.

The resulting spectra are split into 2 parts, the low frequency one, that serves as input to the different models and the high frequency one serves are reference for training the models and computing the evaluation metrics.

Three metrics are considered. The first metric is the average loss on the test set is used to get a global understanding of the behavior of the predictors in the spectral domain. The second metric is the average signal-to-reconstruction ratio (srr), computed in the time domain. Complex spectral values are computed for each batch of texture windows using the original phase for the low frequency part and several estimates for the high frequency part. The phase can be the original phase, termed \textit{oracle} in the following, the low frequency phase as proposed in \cite{miron2018high} and a flipped version of the low frequency phase as proposed in \cite{li2015deep}, termed \textit{mirror} in the following. The latter gave consistently better results, so only the results achieved using this method are reported.

As it will be discussed in Section \ref{sec:experiments}, loss of phase correspondence results lead to the srr being no longer relevant as a performance metric. Ultimately, subjective evaluation by means of listening tests performed by humans is desirable. However, setting up those tests is time consuming and considering the whole datasets as stimuli is impractical. This issue is left for future research. A good alternative is the use of objective metrics based on perceptual models that have been proposed and used with success in many audio signal processing tasks, such as the overall difference grade (odg) of the perceptual evaluation of audio quality tool (peaq) \cite{thiede2000peaq} and the overall perceptual score (ops) of perceptual evaluation methods for audio soure separation tool  (peass) \cite{emiya2011subjective}. The first has the advantage of being part of an ITU standard and the second considers an advanced hair cell model and some other improvements. Both have been considered in this study, but as the two appear to be correlated in those experiments, only the second is reported.

Three baselines are considered. The first is a drastically simplified version of the sbr technique, where the high frequency part is simply the low frequency part whose amplitudes are scaled. Please note that this baseline requires some side information, that is the amplitude scaling factor, defined as the ratio of average amplitude between the high frequency part and the low frequency one, computed for each texture window.

The second and the third ones are reimplementation of the deep architectures presented in \cite{miron2018high}. The \textit{CNN bottleneck} encoder part consists in two convolutional layers with filters of size $(1, F)$ and $(T, 1)$ that independently summarize information in the frequency and time dimensions of the input data respectively. A fully connected layer is then applied to extract a code of $64$. The decoder part mirrors these processing steps to recover a spectrum with the original input size. This model is implemented as described in \cite{miron2018high}, though with different filter sizes as $F=128$ and $T=10$ to match the data format considered in this study. The \textit{CNN stride-2} autoencoder adopts a different strategy where frequency patterns information is extracted at multiple scales using strided convolutions. The first four layers operate independently on each time frame, and are followed by two convolutional layers with square kernels. The number of channels increases linearly with each additional layer. In this study, the first two layers are removed to account for the difference in the size of the frequency dimension in considered spectra. %For both architectures input examples are logarithmically scaled using the function $log_{10}(1+x)$ to account for the magnitude difference between low and high frequencies, as described by the authors.

%\footnote{Note for reviewers, some other preprocessing steps taken in \cite{miron2018high}, like logarithmical scaling and normalization are not implemented as they lead to numerical instabilities which are still under study. We also contacted the authors that will send us the train/test split. This will hopefully allow us to add the medleys dataset to the study.}

Additionally, two anchors are considered: the \textit{oracle} predictor that "predicts" the actual high frequency spectral pattern and the \textit{null} predictor that outputs a vector of zeros.

\section{Experiments}
\label{sec:experiments}

The experiments reported here are conducted in order to study two main design issues for the task at hand:
\begin{itemize}
  \item the impact of the dilation on the other meta parameters;
 \item the impact of compressive architecture by performing a comparative study of a full cnn architecture versus two autoencoder ones.
\end{itemize}


\begin{figure*}[t]
\center
\begin{tabular}{cc}
  (a) & (b) \\
  \includegraphics[width = .95\columnwidth]{figures/solos_1141.png}
&
  \includegraphics[width = .95\columnwidth]{figures/gtzan_1120.png}
\end{tabular}
\caption{Examples of predictions for (a) the solos dataset and (b) the \textit{gtzan} dataset. The proposed approach catches correctly the harmonic structures (a) and the average magnitude of more complex spectral shapes (b).}
\label{fig:spec}
\end{figure*}

\subsection*{Dilation}

The dilation parameter $D$ is a very interesting feature of convolutional network as they allow us to expand the receptive field without increasing the model complexity. The other parameters that controls its size are $K$ the size of the filter, and $L$ the number of layers. As can be seen on Table \ref{tab:kvsd}, $D=2$ allows us to reduce the loss and also to reduce the gain achieved by increasing $K$. The effect is more sensible on the \textit{gtzan} dataset. Table \ref{tab:lvsd} shows the same regularization effect of $D$ on $L$. As predictable, this regularization effect is not present on $C$ (not shown due to space constraints). %see Table \ref{tab:cvsd}.

Extending the dilation factor further, $D=3$, is not beneficial, as the receptive field is sufficiently large with $D=2$ (values not shown due to space constraints). Following this, a compact but effective architecture is selected with $D=2$, $K=17$, $L=6$, and $C=64$.

\fg{formule pour calculer la taille du receptive field, same en temps: input layer + Hidden layers + Output layer}
\begin{equation}
R_f = K_f + (L-2)D\frac{K_f-1}{2} + \frac{K_f-1}{2}
\end{equation}


\begin{table}[t]
  \begin{center}
\begin{tabular}{llcc}
$K$ & dilation ($D$) & gtzan & medleysolos \\
\hline
13 & 1 & 0.109 $\pm$0.020 & 0.237 $\pm$0.014 \\
17 & 1 & 0.105 $\pm$0.021 & 0.226 $\pm$0.016 \\
13 & 2 & 0.102 $\pm$0.021 & 0.226 $\pm$0.017 \\
17 & 2 & 0.102 $\pm$0.022 & 0.218 $\pm$0.019 \\
% 13 & 3 & - $\pm$- &     - $\pm$- \\
% 17 & 3 & 0.103 $\pm$0.022 &     - $\pm$- \\
\end{tabular}
\caption{Spectral loss on the testing set for the proposed architecture with $L=7$ and $C=64$.}
\label{tab:kvsd}
  \end{center}
  \vspace{-4mm}
\end{table}

\begin{table}[t]
  \begin{center}
\begin{tabular}{llcc}
$L$ & dilation ($D$) & gtzan & medleysolos \\
\hline
5 & 1 & 0.110 $\pm$0.017 & 0.240 $\pm$0.013 \\
6 & 1 & 0.107 $\pm$0.021 & 0.231 $\pm$0.016 \\
7 & 1 & 0.105 $\pm$0.021 & 0.226 $\pm$0.016 \\
5 & 2 & 0.102 $\pm$0.019 & 0.228 $\pm$0.015 \\
6 & 2 & 0.102 $\pm$0.022 & 0.225 $\pm$0.019 \\
7 & 2 & 0.102 $\pm$0.022 & 0.218 $\pm$0.019 \\
% 5 & 3 & - $\pm$- &     - $\pm$- \\
% 6 & 3 & - $\pm$- &     - $\pm$- \\
% 7 & 3 & 0.103 $\pm$0.022 &     - $\pm$- \\
\end{tabular}
\caption{Spectral loss on the testing set for the proposed architecture with $K=17$ and $C=64$.}
\label{tab:lvsd}
\end{center}
\vspace{-4mm}
\end{table}

The dilation parameter $D$ is a very interesting feature of convolutional network as they allow us to expand the receptive field without increasing the model complexity. The other parameters that controls its size are $K$ the size of the filter, and $L$ the number of layers. As can be seen on Table \ref{tab:kvsd}, $D=2$ allows us to reduce the loss and also to reduce the gain achieved by increasing $K$. The effect is more sensible on the \textit{gtzan} dataset. Table \ref{tab:lvsd} shows the same regularization effect of $D$ on $L$. As predictable, this regularization effect is not present on $C$ (not shown due to space constraints). %see Table \ref{tab:cvsd}.

Extending the dilation factor further, $D=3$, is not beneficial, as the receptive field is sufficiently large with $D=2$ (values not shown due to space constraints). Following this, a compact but effective architecture is selected with $D=2$, $K=17$, $L=6$, and $C=64$.

\fg{formule pour calculer la taille du receptive field ?}



% \begin{table}[t]
% \begin{tabular}{llcc}
% $C$ & dilation ($D$) & gtzan & medleysolos \\
% \hline
%  32 & 1 & 0.108 $\pm$0.020 & 0.237 $\pm$0.014 \\
%  64 & 1 & 0.107 $\pm$0.021 & 0.231 $\pm$0.016 \\
% 128 & 1 & 0.107 $\pm$0.019 & 0.233 $\pm$0.014 \\
%  32 & 2 & 0.102 $\pm$0.021 & 0.229 $\pm$0.016 \\
%  64 & 2 & 0.102 $\pm$0.022 & 0.225 $\pm$0.019 \\
% 128 & 2 & 0.101 $\pm$0.020 & 0.220 $\pm$0.017 \\
% \end{tabular}
% \caption{Loss on the testing set for the proposed architecture with $K=17$ and $L=7$.}
% \label{tab:cvsd}
% \end{table}

\subsection*{Full cnn versus autoencoders}

Compared to the two encoders baselines, the proposed approach compares favorably in terms of spectral loss, see Table \ref{tab:loss}. An advantage of considering spectral bandwidth extension as task is the ease of fine grain performance analysis. Contrary to classification or generation pipelines the input and the output of the network are equivalent quantitatively and qualitatively speaking. It allow us to visually interpret the behavior of the predictors. As can be seen on Figure \ref{fig:spec}, the proposed approach catches correctly the harmonic structures and the average magnitude of more complex spectral shapes.

\begin{table}[t]
  \begin{center}
\begin{tabular}{lcc}
method & gtzan & medleysolos \\
\hline
proposed & 0.102 $\pm$0.022 & 0.225 $\pm$0.019 \\
autoDense & 0.107 $\pm$0.017 & 0.241 $\pm$0.011 \\
autoStride & 0.110 $\pm$0.017 & 0.228 $\pm$0.013 \\
\end{tabular}
\caption{Spectral loss over the test set.}
\label{tab:loss}
\end{center}
\vspace{-4mm}
\end{table}

In order to evaluate the proposed approach in the time domain, the srr is considered. As can be seen on Table \ref{tab:srr}, there is a direct inverse correlation between the spectral loss and the srr when considering the oracle phase. Considering the mirror phase strongly reduces the phase correlation between the reference and the estimate leading to low srr even for the oracle estimator. Though, informal listening test shows that considering the oracle estimator using the mirror phase estimate is perceptively more pleasing than the null predictor. We conclude that the sensitivity of the srr to phase shift reduces its usefulness and turn to an objective perceptual measure to better interpret the performance of the predictors.

 As can be seen on Table \ref{tab:ops}, the proposed approach improves over the baselines. Whereas the autoencoder architectures appears to perform similarly in terms of srr, they have different behavior when considering the ops metric, the stride being more effective on monophonic music and the dense being more adapted to polyphonic music.\fg{any clues on architectural explanations ? fait toi plaisir hein :)}

 \ml{perform ttest when all computation is done}

\begin{table}
  \begin{center}
\begin{tabular}{lcc|cc}
  & \multicolumn{2}{c}{medleysolos} & \multicolumn{2}{c}{gtzan} \\
  method & mirror & oracle & mirror & oracle \\
\hline
null & 12.9 $\pm$3.1 & 12.9 $\pm$3.1 & 12.9 $\pm$3.3 &  12.9 $\pm$3.3\\
replicate & 10.7 $\pm$2.9 & 11.6 $\pm$2.9 & 10.8 $\pm$3.3 & 13.3 $\pm$3.2\\
\hline
autoDense & 10.6 $\pm$2.5 &  13.3 $\pm$2.8 & 11.1 $\pm$3.0 & 15.6 $\pm$3.2 \\
autoStride & 11.2 $\pm$2.6 & 13.9 $\pm$2.7 & 11.2 $\pm$2.8 & 15.6 $\pm$3.1 \\
\hline
proposed  & 11 $\pm$2.4 & 14.3 $\pm$2.6  & 11.3 $\pm$3.0 & 16.3 $\pm$3.3 \\
\hline
oracle & 10.5 $\pm$3.1 & $\infty$ & 10.5 $\pm$3.3 & $\infty$ \\
\end{tabular}
\caption{Srr achieved by the different methods on the \textit{medely-solosdb} and the \textit{gtzan} datasets using the \textit{mirror} and \textit{oracle} phase estimates.}
\label{tab:srr}
\end{center}
\vspace{-4mm}
\end{table}

% \begin{table}[t]
%   \begin{tabular}{lcc}
%   method   & mirror & oracle \\
%   \hline
%   null & 12.91 $\pm$3.26 &  12.91 $\pm$3.26 \\
%   replicate & 10.84 $\pm$3.30 & 13.30 $\pm$3.22 \\
% \hline
%   autoDense & 11.14 $\pm$2.98 & 15.62 $\pm$3.23 \\
%   autoStride & 11.20 $\pm$2.84 & 15.58 $\pm$3.10 \\
%   \hline
%   proposed  & 11.28 $\pm$2.98 & 16.32 $\pm$3.31\\
%   \hline
%   oracle & 10.51 $\pm$3.26 & $\infty$ \\
%   \end{tabular}
%   \caption{Srr achieved by the different methods on the \textit{gtzan} dataset using the \textit{mirror} and \textit{oracle} phase estimates.}
%   \label{tab:srrGtzan}
% \end{table}


\begin{table}[t]
\begin{center}
\begin{tabular}{lcc}
method & medleysolos & gtzan \\
\hline
proposed & $\pm$ & 42.9 $\pm$5.7 \\
autoDense & 31.7 $\pm$3.2 & 40.6 $\pm$5.8 \\
autoStride & 33.3 $\pm$2.7 & 36.9 $\pm$ 5.7 \\
\end{tabular}
\caption{Overall perceptual score.}
\end{center}
\label{tab:ops}
\vspace{-4mm}
\end{table}

\section{Discussion}
\label{sec:discussion}

The benefit of considering a dilated full convolutional neural network for the task of predicting the high frequency part of the magnitude spectra using the low frequency part have been studied. The proposed architecture has been thoroughly evaluated in terms of several metrics, from the optimized loss to an objective perceptual measure.

Considering the Fourier spectrum as input has several drawbacks. Time/Frequency resolution tradeoff and phase estimation inherently limit the potential of the proposed approach. Future work will considered more advanced phase estimators as well as sample based deep architectures. The dependency to the training set on the behavior of the predictors will also be studied.

%We believe that considering the bandwidth extension task has several benefits for gaining knowledge on the behavior of deep neural architecture and seek to pursue this line of research.



Alongside those technical issues, we also believe that considering this task with varied learning and testing sets, \textit{i.e.} predicting the high frequency spectra of a 'pop' song using network trained on 'country' songs may be of creative interest to explore yet to be defined formal definitions of musical style transfer \cite{dai2018music}.


\vfill\pagebreak


\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}
